# Site
repository: sproogen/resume-theme
favicon: images/favicon.ico

# Content configuration version
version: 2

# Personal info
name: Jinki Lee
title: Senior AI Engineer – Inference & Performance Optimization
description: 
email: jinkilee73@gmail.com
location: Korea
website: jinkilee.github.io

# Dark Mode (true/false/never)
darkmode: false

# Social links
#twitter_username: facespics
github_username:  jinkilee
# stackoverflow_username: "00000001"
# dribbble_username: jekyll
# facebook_username: jekyll
# flickr_username: jekyll
instagram_username: jinkilee73
linkedin_username: jinki-lee-355546b1
# xing_username: jekyll
# pinterest_username: jekyll
# youtube_username: globalmtb
# googleplus_username: +jekyll
# orcid_username: 0000-0000-0000-0000

# Additional icon links
#additional_links:
#- title: itsgoingto.be
#  icon: fas fa-globe
#  url: https://www.itsgoingto.be
# - title: another link
#   icon: font awesome brand icon name (eg. fab fa-twitter) (https://fontawesome.com/icons?d=gallery&m=free)
#   url: Link url (eg. https://google.com)

# Google Analytics and Tag Manager
# Using more than one of these may cause issues with reporting
# gtm: "GTM-0000000"
# gtag: "UA-00000000-0"
# google_analytics: "UA-00000000-0"

# About Section
# about_title: About Me
about_profile_image: images/jinki.jpg
about_content: | # this will include new lines to allow paragraphs
  Senior AI Engineer specializing in low-latency, high-throughput inference systems
  with GPU and NPU optimization for production environments.
  
  Performance-oriented AI engineer specializing in low-latency, high-throughput inference systems across GPU and NPU accelerators in real-world production environments.
  
  I design and optimize end-to-end inference pipelines by identifying system-level bottlenecks across preprocessing, model execution, and postprocessing. My work focuses on async execution, accelerator-aware pipeline design, and low-level optimization to deliver measurable latency reduction and cost efficiency.
  
  I have deployed real-time inference systems in safety-critical and resource-constrained environments, achieving **over 50% infrastructure cost savings** through heterogeneous GPU/NPU strategies.

content:
  - title: Projects
    layout: list
    content:
      - layout: top-middle
        title: Edge-based Multi-Accelerator Inference System
        quote: >
          GPU Usage Maximization??
        description: | # this will include new lines to allow paragraphs
          I developed a real-time safety monitoring inference system operating under tight latency constraints and hardware limitations. The solution leveraged asynchronous inference pipelines designed for heterogeneous GPU and NPU execution, allowing flexible deployment across edge environments. Performance was further optimized through TensorRT asynchronous execution and kernel fusion, GPU-resident postprocessing via custom TensorRT plugins, and mixed CPU/GPU preprocessing using NVIDIA DALI to minimize data transfer and synchronization overhead. This architecture enabled stable real-time inference, substantially reduced latency, and resulted in more than 50% savings in hardware infrastructure costs.
          
    content:
      - layout: top-middle
        title: High-Throughput Vision Inference Pipeline
        quote: >
          CPU/GPU Usage Maximization??
        description: | # this will include new lines to allow paragraphs
          I optimized a high-throughput vision inference pipeline that suffered from CPU-bound preprocessing and postprocessing bottlenecks. The pipeline was rearchitected to maintain GPU residency of data across preprocessing, inference, and postprocessing stages, reducing synchronization overhead and host-device transfers. Performance-critical components were reimplemented in C++ and seamlessly integrated into the Python-based inference workflow, while unnecessary CPU–GPU synchronization points were removed. As a result, the system achieved a notable improvement in throughput and more stable P99 latency in real-time inference scenarios.
          

  - title: Experience # Title for the section
    layout: list # Type of content section (list/text)
    content:
      - layout: left
        title: PoscoDX
        link: www.poscodx.com
        sub_title: Senior AI Engineer
        caption: October 2019 - Present
        quote: >
            "Focusing on AI deployment to optimize AI hardware and reduce business cost"
        description: | # this will include new lines to allow paragraphs
            As a Senior AI Engineer at POSCO DX, I designed and optimized real-time AI inference pipelines for industrial safety and monitoring systems operating under strict latency and reliability requirements. I reduced end-to-end latency by restructuring asynchronous CPU–GPU execution flows and eliminating synchronization bottlenecks, while improving GPU utilization through TensorRT kernel fusion and execution graph optimization. Performance-critical postprocessing was fully offloaded to the GPU via custom TensorRT plugins, and mixed CPU/GPU preprocessing pipelines were implemented using NVIDIA DALI to overlap I/O and computation. I also integrated NPU-based inference by modifying ONNX graphs and adapting execution pipelines to accelerator-specific constraints. These systems were deployed across domestic industrial sites, overseas subsidiaries, and airport environments, resulting in stable real-time operation and more than 50% reduction in hardware infrastructure costs through heterogeneous accelerator strategies.
      
      - layout: left
        title: Ahnlab
        link: www.ahnlab.com
        sub_title: Junior AI Researcher
        caption: October 2016 - October 2019
        quote: >
            "Lots of ML researches for cyber security industry lead me to AI world, which made me to focus utilize AI as an automation tool."
        description: | # this will include new lines to allow paragraphs
            I re-joined Ahnlab in 2016 as an AI researcher and performed many ML and AI researches to apply automation on SOC operation. Especially, I applied the idea of pre-training and fine-tuning concept on security log domain and developed a BERT model for security domain which was trained from scratch. The pre-trained BERT model for security domain was fine-tuned to be able to correctly classify pre-defined threats.
            
            In SOC, my classification model for pre-defined threats could automate partial regular security analysis, which could reduce human resource for regular jobs.
      
      - layout: left
        title: Ahnlab
        link: www.ahnlab.com
        sub_title: Junior Security Analyst
        caption: July 2012 - Auguest 2015
        quote: >
            "Though this experience is far from AI, I believe that overall computer science knowledge and experience that I had in this term made me much skillful AI Enginner."
        description: | # this will include new lines to allow paragraphs
            I joined Ahnlab in 2012 as a junior security analyst in 2012. I performed as a operator at Security Operation Centre, i.e. SOC. I analyzed real-time incoming network security logs from FW, WAF and IPS to protect customer's server assets until 2015. 
                      
  - title: Publication # Title for the section
    layout: list
    content:
      - layout: left
        title: 안녕, 트랜스포머
        caption: 2023 - 2024
        sub_title: all about transformer architecture
        description: |
          I authored Hello, Transformer, a technical book written prior to the widespread adoption of large language models, aimed at introducing transformer architectures from first principles. The book focuses on core architectural concepts, implementation details, and practical code examples, enabling readers to build a strong foundational understanding of transformer-based models. Throughout the writing process, I continuously refined the content by validating concepts through hands-on experiments and maintaining accompanying code examples, strengthening both my technical depth and ability to communicate complex systems clearly.
          
  - title: Education # Title for the section
    layout: list
    content:
      - layout: left
        title: University of Warwick
        #caption: 2015 - 2016
        #sub_title: MSc Data Analysis
        #quote: >
        #  Established in 1636, Harvard is the oldest higher education institution in the United States, and is widely regarded in terms of its influence, reputation, and academic pedigree as a leading university in not just the US but also the world.
        description: | # this will include new lines to allow paragraphs
          - 2015-2016
          - MSc Data Analysis
          
      - layout: left
        title: Sungkyunkwan University
        #caption: 2008 - 2012
        #sub_title: BSc Computer Science
        #quote: >
        #  Established in 1636, Harvard is the oldest higher education institution in the United States, and is widely regarded in terms of its influence, reputation, and academic pedigree as a leading university in not just the US but also the world.
        description: | # this will include new lines to allow paragraphs
          - 2008 - 2012
          - BSc Computer Science

          
  - title: Skills
    layout: list # Type of content section (list/text)
    content:
      - layout: left
        title: Python
        caption: numpy, opencv, ultralytics, FastAPI
        description: | # this will include new lines to allow paragraphs
          - Optimizing multiple thread execution over GIL
          - Acceleration using multiprocessing
          - Async execution
          
      - layout: left
        title: C++
        description: | # this will include new lines to allow paragraphs
          - Writing custom python package in C++ to accelerate CPU usage or leaverage concurrency
          - Thread execution to increase concurrency
          
      - layout: left
        title: GPU
        description: | # this will include new lines to allow paragraphs
          - TensorRT in C++/Python
          - Triton Inference Server leaveraging dynamic batching
          - Writing a custom TensorRT Plugin using CUDA programming
          - NsightSystems/Nsight Compute for profile GPU usage
      
      - layout: left
        title: Others
        description: | # this will include new lines to allow paragraphs
          - Accustomed to Docker environment
          - Experience over different type of accelerator such as NCS2 and NPU
          - Network packet anaysis, i.e. wireshark


#  - title: A Little More About Me
#    layout: text
#    content: | # this will include new lines to allow paragraphs
#      Alongside my interests in networks and software engineering some of my other interests and hobbies are:
#      - Rock climbing
#      - Gaming
#      - Knitting
#      - [Becoming a ninja](https://www.youtube.com/watch?v=vtg4o__aRMg)
#
#      Look at this cool image  
#      ![Trees](images/landscape-trees.jpg "Trees")

# Footer
footer_show_references: true
# references_title: References on request (Override references text)

# Build settings
remote_theme: sproogen/resume-theme

sass:
  sass_dir: _sass
  style: compressed

plugins:
 - jekyll-seo-tag
